---
title: "Isn't that a TikTok song?"
author: "Anna Lau"
date: "February-March 2020"
# runtime: shiny
output:
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: "yeti"
    css: css_storyboard.css
    # logo: images/tiktok_logo.png
    favicon: images/tiktok_logo.png
# output: html_document
---
```{r, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```
```{r, echo = FALSE, result = FALSE, warning = FALSE}
library(spotifyr)
library(tidyverse)
library(plotly)
library(grid)
library(crosstalk)
library(gridExtra)
library(compmus)
library(dplyr)
library(devtools)
library(tidyr)
library(tidyverse)
library(tidymodels)
library(ggdendro)
library(ggalt)
library(heatmaply)
library(spotifyr)
library(compmus)
library(shiny)
```

```{r}
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
} 

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```
```{r}
mar <-get_playlist_audio_features("","0IIt2QOYxRawDHQbZ8ln3M") 
apr <-get_playlist_audio_features("","0PTklfVTalxE8j6T9NYZEG") 
may <-get_playlist_audio_features("","093VTj7Esn9nYiFQFoa1YH") 
jun <-get_playlist_audio_features("","2F3YMerEwMjXn3A4S7es6H") 
jul <-get_playlist_audio_features("","4U9w6uCKivCsRLHFyZgkxI") 
aug <-get_playlist_audio_features("","3zvNyJbb9rjdF0vag2O5xG")
sep <-get_playlist_audio_features("","2APegDsKij3yaL1ffG6Bz4") 
oct <-get_playlist_audio_features("","4np9mWC6T4O8x7hoxTeUPP") 
nov <-get_playlist_audio_features("","5xyKvHZIeWGAgWJ6ZEVm3H") 
dec <-get_playlist_audio_features("","4n90Xr0lxy1yLhbzTAgzTx") 

corpus <- 
  bind_rows(
    mar %>% mutate(category = "Mar"),
    apr %>% mutate(category = "Apr"),
    may %>% mutate(category = "May"),
    jun %>% mutate(category = "Jun"),
    jul %>% mutate(category = "Jul"),
    aug %>% mutate(category = "Aug"),
    sep %>% mutate(category = "Sep"),
    oct %>% mutate(category = "Oct"),
    nov %>% mutate(category = "Nov"),
    dec %>% mutate(category = "Dec")
  )

# Delete duplicate songs
corpus <- corpus[!duplicated(corpus$track.name),]
pop_sorted <- corpus[order(-corpus$track.popularity),]

corpus$trend = c("dance","dance","dance","dance","transformation", "random", "dance","transformation","humour", 
"humour","dance","dance","humour","story telling","random","transformation","dance","humour","dance","dance",
"dance","dance","transformation","dance","humour","dance","transformation","dance", "dance","random","dance",
"dance","humour","random","story telling","dance","story telling","dance","dance","dance","dance","dance",
"humour","dance","story telling","humour","dance","random","dance","dance","dance","story telling","dance", 
"dance","dance","dance","dance","random","dance","story telling","transformation","dance","dance","random",
"dance","random","random","random","random","humour","dance","story telling","dance","story telling","transformation",
"random","random","dance","dance","random","story telling","dance","dance")

corpus <- corpus %>%
  mutate(trends = ifelse(trend == "dance", "Dance", "Other"))

corpus<- corpus %>% mutate(poprange = case_when(
    between(track.popularity, 0, 50) ~ "Low",
    between(track.popularity, 51, 75) ~ "Middle",
    between(track.popularity, 76, 100) ~ "High",
    TRUE ~ NA_character_
  ))
```


<!-- ### Okay... So is there anything specific about these trends? Tell me what is the *trend* of this TikTok song? -->

```{r}
# corpus_features <-
#   corpus %>%  # For your portfolio, change this to the name of your corpus.
#   add_audio_analysis() %>% 
#   mutate(
#     playlist = factor(trends),
#     segments = map2(segments, key, compmus_c_transpose),
#     pitches =  map(segments,
#         compmus_summarise, pitches,
#         method = "mean", norm = "manhattan"
#       ),
#     timbre =
#       map(
#         segments,
#         compmus_summarise, timbre,
#         method = "mean",
#       )
#   ) %>%
#   mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
#   mutate_at(vars(pitches, timbre), map, bind_rows) %>%
#   unnest(cols = c(pitches, timbre))
# ```
# ```{r}
# corpus_recipe <-
#   recipe(
#     playlist ~
#       danceability +
#       energy +
#       loudness +
#       speechiness +
#       acousticness +
#       instrumentalness +
#       liveness +
#       valence +
#       tempo +
#       duration +
#       C + `C#|Db` + D + `D#|Eb` +
#       E + `F` + `F#|Gb` + G +
#       `G#|Ab` + A + `A#|Bb` + B +
#       c01 + c02 + c03 + c04 + c05 + c06 +
#       c07 + c08 + c09 + c10 + c11 + c12,
#     data = corpus_features,          # Use the same name as the previous block.
#   ) %>%
#   step_center(all_predictors()) %>%
#   step_scale(all_predictors())      # Converts to z-scores.
#   # step_range(all_predictors())    # Sets range to [0, 1].
```
```{r}
# corpus_cv <- corpus_features %>% vfold_cv(5)
```
```{r}
# knn_model <-
#   nearest_neighbor(neighbors = 1) %>%
#   set_mode("classification") %>% 
#   set_engine("kknn")
# corpus_knn <- 
#   workflow() %>% 
#   add_recipe(corpus_recipe) %>% 
#   add_model(knn_model) %>% 
#   fit_resamples(
#     corpus_cv, 
#     control = control_resamples(save_pred = TRUE)
#   )
```
```{r}
# corpus_knn %>% get_conf_mat()
```
```{r}
# corpus_knn %>% get_conf_mat() %>% autoplot(type = "mosaic")
```

<!-- *** -->

<!-- So yeah we cannot really predict what songs are supposed to fall into the *Dance* category and which fall into the *Other* category. -->




<!-- ### Homework week 10 - Tempogram -->

```{r}
# play_date <- get_tidy_audio_analysis("4DpNNXFMMxQEKl7r0ykkWA")
# ```
# ```{r}
# play_date_cyclic <- play_date %>%
#   tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
#   ggplot(aes(x = time, y = bpm, fill = power)) +
#   geom_raster() +
#   scale_fill_viridis_c(guide = "none") +
#   labs(x = "Time (s)", y = "Tempo (BPM)", title = "Play Date - Melanie Martinez (Cyclic)") +
#   scale_x_continuous(breaks = seq(0,180, 30) ) + 
#   geom_vline(xintercept = 132, color = "white")+
#   geom_vline(xintercept = 136, color = "white")+
#   geom_vline(xintercept = 165, color = "white")+
#   theme_classic()
# 
# play_date_cyclic
```

<!-- *** -->

<!-- The tempogram of the song *Play Date* by Melanie Martinez, shows that there are small tempo variations throughout the entire song. Very notable is the change in tempo between 132 - 136 seconds(2:12 - 2:16). The part in the song corresponding to this time frame is actually almost silent. It is surprising that Spotify labels this silent part to have a high tempo. At the end of this time frame slow beating comes in again, hence we see the yellow line in the time frame go down again. Once after we see that the tempo goes back to the usual. At the very end, the song slows down and fades out, this corresponds to the last few seconds where we see that the tempogram is slightly lighter again.  -->


### Yeah, **you've heard this song before**, but where did it come from? {data-commentary-wdith=500px}
The corpus for this project is a playlist from Spotify. [This playlists](https://open.spotify.com/playlist/5kQyvWFk8r1y2IQEtjBfey?si=HLNnUJzYQHSVYq7ub3lbRA) is a collection of songs that are famous from TikTok - a social media app used to make short-form videos. In these 15 seconds videos, music is played and people dance, lip-sync, tell stories, make humorous memes, and do all other kinds of trends. As the spaces people normally enjoy music were impacted by the pandemic, TikTok helped fill the need for communal musical experience all over the world. Starting from the beginning of the pandemic, March 2020, TikTok gained a lot of popularity. Hence the songs from the corpus will be the top 10 songs on TikTok from March 2020 till December 2020. The top songs are acquired from [this website](https://tokboard.com/months). 

I want to find out how different these popular TikTok songs are from one another. Altogether, the genres of the songs are quite the same. So I would assume that there is a factor that is a core element to becoming a suitable TikTok song. In addition, as TikTok videos have a wide variety of trends, I want to find out what the similarities and differences are within a trend. More specifically I will be focusing on the *Dance* trend as that is the biggest on TikTok.

<!-- Interesting comparison points in the corpus of this project will be the overall mood of a song. Psychologist Robert Thayer's came up with a traditional model of mood. Where songs can be categorised along the lines of energy and stress, from happy to sad and calm to energetic (Bhat et al 359). Next to these 4 categories there are also 4 sub categories e.g. Happy - energetic, Happy - calm, etc. I assume the corpus has a lot of Happy, Energetic, Happy - Energetic and a few Happy - Calm and Calm songs. The reason for that is because many songs are for funny and trendy TikTok-challenges. It is expected that the intensity for Energetic and Happy - Energetic songs are on the higher spectrum than Happy - Calm and Calm songs. Another musical component that can be looked at is the Timbre of a song. Timbre is the tonal quality of a piece created by harmonics. It is to no surprise that these will be different between moods as well. Happy, Happy - Energetic and Energetic songs will have around a medium timbre. Whereas Calm and Happy - Calm songs will most likely have a low timbre. The pitch intervals of the songs will probably vary quite a lot between the mood categories, as it is very dependent on the singer's voice, and the instruments that are used. Lastly the rhythm of songs with a Happy, Happy - Energetic and Energetic mood will most likely be high paced, whereas Calm and Calm - Happy songs will have a more relaxed and slow rhythm.  -->


Lastly, a final note needs to be made on the length of the TikTok videos. These videos can only be up to 15 seconds long. However, in this research the full length of the songs are used. The motivation behind this is because 15 seconds do not allow for a thorough and detailed analysis between the songs. 

**Atypical songs in the corpus**

* *Eine kleine Nachtmusik K. 252 Allegro* - Wolfgang Amadeus, stands out a lot from the rest. It is the only classical piece in the corpus and is composed in the year 1787. 
* *Steven Universe* - L.Dre: one of the few slow paced songs, and it is only 2.14 minutes long  without lyrics. 
* *Monkeys Spinning Monkeys* - Keving MacLeod, this is one of the few songs that have no lyrics and it is also very short.
* Funny to see that from December there are seasonal songs e.g. *All I Want for Christmas is You* - Mariah Carey and *Jinge Bell Rock* - Bobby Helms. 

**Note**
The Song Titles in the table are copied from [the website](https://tokboard.com/months)

***

Here is the playlist that represents the corpus:
<iframe src="https://open.spotify.com/embed/playlist/5kQyvWFk8r1y2IQEtjBfey" width="280" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

**Making the playlist** <br>
If a song in the top 10 of the month was not available on Spotify, the song is skipped and the 11th song was added. If a song was repeatedly listed in the top 10 the song was skipped. This resulted in a total of 83 songs with a total duration of 4 hours and 9 minutes. 

**Setting up the trends per song** <br>
The majority of the songs are associated with dance challenges, hence these songs are categorised under the *Dance* trend. All the other trends are referred to as *Other*.

<details>
<summary>**↓Specification of the corpus selection↓**</summary>
TikTok is a very creative platform where people use all kinds of remix songs. Some of which are made by not well known artists. This leads to that some remixes aren't accessible on Spotify. In the following table popular songs that were *not* listed on Spotify are shown. Here the month indicates when they were popular on TikTok and the Rank shows the popularity rank of the song in that month. This information is acquired from [this website](https://tokboard.com/months).

| Song Title                                | Month    | Rank |
|-------------------------------------------|----------|:----:|
| #hiteverybeat                             | March    | 3    |
| Over it Chinese New year Remix by JohhnyG | March    | 4    |
| 2liveSoundsmix                            | March    | 7    |
| Original sound - KTM                      | March    | 9    |
| Kanta Laga Mix                            | April    | 9    |
| Original sound - Plks choudhary           | April    | 10   |
| Cari Mama Muda                            | May      | 7    |
| Kream bebiisan edition                    | May      | 8    |
| original sound                            | May      | 11   |
| ओरिजिनल साउंड                              | June     | 6    |
| Gimme clout pls                           | June     | 7    |
| Original sound                            | October  | 1    |
| Viva La Swing                             | October  | 5    |
| ... is sweaty                             | November | 8    |

Some songs are listed multiple times in the top ten of the months, these are listed in the following table <br/>

| Song Title                |         Months       | Repetition |
|-------------------------- | :-------------------:|:----------:|
| Savage                    |   Mar - Apr - May    |     3      |
| Roses                     |      Mar - Apr       |     2      |
| Laxed (Siren Beat)        |      Apr - May       |     2      |
| Play Date                 |      Apr - May       |     2      |
| Coño                      |      May - Jun       |     2      |
| Love Story                |      Jul - Aug       |     2      | 
| How you like that         |      Jul - Aug       |     2      |
| WAP                       |      Aug - Sept      |     2      | 
| Monkeys Spinning Monkeys  |Jul - Oct - Nov - Dec |     4      | 
| Oh no                     |      Nov - Dec       |     2      | 
| It's Tricky               |      Nov - Dec       |     2      | 
| Classical Music           |   Aug - Oct - Nov    |     3      | 
| Adderall                  |      Nov - Dec       |     2      |

</details>

### Are you in the **mood**, because I certainly am. Here is how TikTok plays with your **Emotions**.{data-commentary-width=500px}

```{r}

happy <- corpus %>% filter(valence >= 0.5 & energy >= 0.5)
angry <- corpus %>% filter(valence < 0.5 & energy >= 0.5)
calm <-  corpus %>% filter(valence >= 0.5 & energy < 0.5)
sad <-   corpus %>% filter(valence < 0.5 & energy < 0.5)

whaa <- 
  bind_rows(
    happy %>% mutate(emotion = 'Happy'),
    angry %>% mutate(emotion = 'Angry'),
    calm %>% mutate(emotion = 'Calm'),
    sad %>% mutate(emotion = 'Sad')
  )
whaa <- whaa %>% rename(Track = track.name)

sharedCorpus <- SharedData$new(whaa)
filter_trend <- filter_checkbox("trend", "Trend", sharedCorpus, group = ~trends)
filter_emotions <- filter_checkbox("emotions", "Emotion", sharedCorpus, group = ~emotion)

emotion_plot <- ggplot(sharedCorpus, aes(x = valence, y = energy, size = danceability, label = Track)) +
  geom_point(aes(shape = trends, color = trends )) + 
  scale_color_manual(values = c("#9FBEE1", "#1E3D60"))+
  theme_light() +
   scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.5, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.5, 1),
    minor_breaks = NULL
  ) +
  geom_vline(xintercept = 0.5, linetype="dotted", color = "black", alpha = 0.7) +
  geom_hline(yintercept = 0.5, linetype="dotted", color = "black", alpha = 0.7) +
  annotate("text", 0.05, 0.05, label = "Sad", color = "black", alpha = 0.7, fontface="bold") +
  annotate("text", 0.05, 0.95, label = "Angry", color = "black", alpha = 0.7, fontface="bold") +
  annotate("text", 0.95, 0.05, label = "Calm", color = "black", alpha = 0.7, fontface="bold") +
  annotate("text", 0.95, 0.95, label = "Happy", color = "black", alpha = 0.7, fontface="bold") +
  labs(x = "Valence", y = "Energy", title = "The mood of the song", shape = "Trends", color = "Trends", label = "Track") +
  guides(size = FALSE)

bscols(ggplotly(emotion_plot, tooltip = "Track"))

```

***

<details>
<summary>**↓ Filters ↓**</summary>
  `r bscols(widths = c(1,1),list(filter_trend, filter_emotions))`
</details>

So how positive and intense are the songs in the corpus? We can gauge this by using the Valence and Energy values. And what is more, we can categorise songs by emotions using these two values as well. This idea comes from the **valence-arousal model of emotion** developed by J. Russell, but I won't bother you with the details. All that you need to know, is that the model suggests that emotions are distributed along these two dimensions. The emotions are incorporated into the plot.

Whenever I am endlessly scrolling on TikTok, I always get a good laugh out of it or I get really pumped up by all the good vibes from the videos. Hence, I'd assumed that the most to least songs with a certain emotion would be Happy, Calm, Sad, and Angry. As expected, **the amount of Happy songs is the largest**. Contrasting, I am surprised about Angry being second place. Maybe the Spotify API's valence does not exactly line up with the valence as described by the model.

Hovering over the plot, we can identify the name of the tracks. In the Sad quadrant we see a song with a very low Energy, *Steven Universe*. As we'd indicated, this is **one of our atypical songs** in the corpus. Similarly, in the Calm quadrant, we have *Monkeys spinning Monkeys* which we'd also identified as an atypical song. I got to admit this song is something else. You know, when you have a favourite song and you play it **over and over**. Well, with this monkey song, you don't want to do that. At first the song can be quite calming indeed, but to listen to the entire 2:05 minutes? It will drive you insane and the song would better fit in the Angry quadrant.

Focusing on the different trends, we can see that the majority of Happy songs are Dance songs. Sad songs fall more often in the Other trend. Lastly, Angry and Calm songs have about a 1:1 distribution regarding trends. 

And you might wonder, why are some dots larger then others? Good question. The **size indicates the danceability** - another feature of the Spotify API - of the song. A quick glance shows us that the sizes aren't that much different between the two trends. 

### Okay but what about those **15 seconds**, are they any **different** than the rest of the song? A glance at two songs based on timbre.{data-commentary-width=500}

```{r}
lemonade <-
  get_tidy_audio_analysis("02kDW379Yfd5PzW5A6vuGt") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean", norm = "manhattan"              # Change summary & norm.
      )
  )

lemon_ssm_timbre <- lemonade %>%
  compmus_self_similarity(timbre, "euclidean") %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Lemonade Timbre") + 
  # annotate("text", 27.5, 7, label = "Video", color = "white", size = 3, fontface="bold") + 
  geom_vline(xintercept = 24, color = "white") +
  geom_vline(xintercept = 31, color = "white")+ 
  annotate("segment", x = 55, xend = 27.5, y = 50, yend = 50, colour = 'white',
           arrow = arrow(angle = 25, type = "closed", length = unit(2, "mm"))) +
  annotate("text", x = 85, 50, label = "Video", color ="white", fontface = "bold", size = 3)

wipe <-
  get_tidy_audio_analysis("2kefHjNepGePZG4WxH2Vh4") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean", norm = "manhattan"              # Change summary & norm.
      )
  )

wipe_ssm_timbre <- wipe %>%
  compmus_self_similarity(timbre, "euclidean") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "#WIPEITDOWN Timbre") +
  geom_vline(xintercept = 1, color = "white") +
  geom_vline(xintercept = 15, color = "white") + 
  annotate("segment", x = 20, xend = 7.5, y = 50, yend = 50, colour = 'white',
           arrow = arrow(angle = 25, type = "closed", length = unit(2, "mm"))) +
  annotate("text", x = 35, 50, label = "Video", color ="white", size = 3, fontface = "bold")

grid.arrange(lemon_ssm_timbre, wipe_ssm_timbre, ncol = 2, nrow = 1,
             top = textGrob("Self-similarity matrices",gp=gpar(fontsize=20,font=1)))

```

***

Now that we've looked at the emotions of the songs, we are going to explore the timbre of two different songs. Timbre is often use as an indicator why certain sounds sound differently. 

I've chosen the tracks *Lemonade* by Internet Money and *#WIPEITDOWN* by BMW KENNY. I picked these two songs as they had similar values for Spotify's features and are both archetypes as TikTok songs. The *Lemonade* track is associated with the Dance trend, whereas the *#WIPEITDOWN* is associated with the Other trend. 

As we all might know, TikTok consists of many short videos typically of 15 seconds. What I wanted to find out with these plots, is if those specific ±15 seconds are special within a track. For this we'll dive a bit deeper into these Self-similarity matices (SSMs).

Let's start with the SSM for our **Dance track** - *Lemonade*. On this plot we can sort of see a large darker square surrounded with some light sides. What this simply indicates is that overall the middle of the song has quite a homogeneous character. The lighter parts - the beginning and ending - have a different character than the middle. Moreover, they even have a different character from each other as the ending is lighter than the beginning. The time frame that is used for the video falls within that large darker square, so it is actually quite **a representative section** of the song. 

Moving on to our SSM for *#WIPEITDOWN*, we see a very clear **checkerboard pattern.** What this pattern indicates is that there are parts in the song that are similar to one another, but not constantly similar like in our SSM for *Lemonade*. We only see one clear light strip in the entire SSM for this song, so this indicates that this part is quite different than the rest. And it happens to fall within the first half of the time frame of our TikTok video. The other half is more repeated throughout the song as we see more similar colours.

### That is a **classic** for sure... or is it not entirely **in tune** with the rest?{data-commentary-width=500}

```{r}
classical <-
  get_tidy_audio_analysis("2mRUmSG3XGjFloqgAT2UJN") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

savage <-
  get_tidy_audio_analysis("4Oun2ylbjFKMPTiaSbbCih") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```
```{r}
classical %>%
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if desired
    method = "angular",  # Try different distance metrics
    norm = "euclidean"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", title = "Eine Kleine Nachtmusik K. 525. Allegro: A Keygram") +
  geom_hline(yintercept = "G:maj", color = "white") +
  geom_hline(yintercept = "D:maj", color = "white")  +
  geom_hline(yintercept = "C:min", color = "red", alpha = 0.5)+
  geom_hline(yintercept = "E:maj", color = "red")+
  # geom_vline(xintercept = 150, color = "white")+
  # geom_vline(xintercept = 195, color = "white") + 
  geom_segment(aes(x =150, xend = 195, y="F:maj", yend= "F:maj"), col = 'orange') +
  geom_segment(aes(x =150, xend = 195, y="Eb:maj", yend= "Eb:maj"), col = 'orange') + 
  geom_segment(aes(x =150, xend = 195, y="F#:min", yend= "F#:min"), col = 'orange')
```

***

Unlike *Lemonade* and *#WIPEITDOWN* as archetypes of the corpus, *Eine kleine Nachtmusic K. 525 Allegro* by Wolfgang Amadeus Mozart is a special snowflake as it is the only track that falls within the genre of **classical music**. So it isn't quite in tune with all the other TikTok songs. On this page, we will look if the Spotify API's key estimation is properly adjusted to the actual keys of the song.

On the [Wikipedia website](https://en.wikipedia.org/wiki/Eine_kleine_Nachtmusik) of this well-known song, it states that this piece has four movements i.e. G major - D major - Ambiguous key - G major, which are indicated with horizontal white stripes. However, if we look in the keygram we see that the piece is found to be written in E major, the red line. The E major key is most similar to C# minor - the duller red line. However, this chord isn't that much darker than all the other keys. A contrasting part starts at time steps 150 and ends at 195. We see that the keys F major, Eb major and F# minor (short orange lines) are darker coloured within that time frame. Not surprising, as the recapitulation of the classical piece is played. The recapitulation is often the general melody, slightly modified and often played in a **different key**. 

Overall there are several reasons why this key estimation did not work out as expected. For starters, it could be the case that the song is **not pitched exactly around 440 Hz**, this offsets the estimation for Spotify's API. Secondly, string instruments deal with **vibrato**. So this means that there is no exact frequency, rather it is blurred as there is a constant modulation in frequency. For key estimation, one needs to look at the different pitches of a song. Meaning, the combination of 4 string instruments that don't play the perfect pitch of notes - due to vibrato - can also result in the Spotify API being incapable of the key estimation of this particular song. 


### Did you find the **key** of what makes a TikTok song suitable for **Dance VS Other** trends? Maybe we should pump up the beat because the **tempo** is too slow. {data-commentary-width=500}

```{r}
dance_counts <- corpus %>%
  filter(trends == "Dance")%>%
    count(key_name) %>%
      mutate(perc = n/sum(n), trends = "Dance")

other_counts <- corpus %>%
  filter(trends == "Other")%>%
    count(key_name) %>%
      mutate(perc = n/sum(n), trends = "Other")

key_hist <- rbind(dance_counts, other_counts) %>%
  ggplot(aes(x = key_name, y = perc)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  facet_wrap(~trends)+
  labs(title = "Distribution of keys",
           x = "Key",
           y = "Normalised Frequency") + theme_light() +
  geom_hline(yintercept = 0.075, colour = "purple")

key_hist2 <- corpus %>%
  ggplot(aes(x = key_name)) +
  geom_histogram( fill = "skyblue") +
  facet_wrap(~trends)+
  labs(title = "Distribution of keys",
           x = "Key",
           y = "Frequency") + theme_light() +
  geom_hline(yintercept = 5, colour = "purple")
  
dance_temp <- corpus %>%
  filter(trends == "Dance") %>%
  summarise(
    mean_tempo = mean(tempo),
    sd_tempo = sd(tempo)
  )

other_temp <- corpus %>%
  filter(trends == "Other") %>%
  summarise(
    mean_tempo = mean(tempo),
    sd_tempo = sd(tempo)
  )

data <- data.frame(
  trends = c("Dance", "Other"),
  mean = c(dance_temp$mean_tempo, other_temp$mean_tempo),
  tempo = c(dance_temp$mean_tempo+5, other_temp$mean_tempo+5),
  y = c(4,4)
)

dummy2 <- data.frame(trends = c("Dance", "Other"), Z = c(dance_temp$mean_tempo, other_temp$mean_tempo))

tempo_hist<- corpus %>%
  ggplot(aes(x = tempo)) +
  geom_histogram(binwidth = 1, fill = "skyblue", alpha = 0.8) +
  facet_wrap(~trends) +
  scale_x_continuous(breaks = seq(60, 180, 30))+
  labs(x = "Tempo (BPM)", y = "Frequency", title = "Distribution of tempo") + theme_light()

what<- tempo_hist + geom_vline(data = dummy2, aes(xintercept = Z), colour = "purple") +
  geom_text(data = data, size = 3, aes(x = tempo, y = y,label = round(mean, digits = 0))) 

grid.arrange(key_hist, what, nrow =2, ncol = 1)
```

***

Zooming out from the *Eine Kleine Nachtmusik K. 525. Allegro* track, let's have a look at the over usage of keys in the corpus. Given the trends in the corpus, these normalised histograms are made based on the key that was found by the Spotify AP, which can be seen on the first row. 

Overall we can see that there is quite some similarity between the different trends. For example both trends indicate that within the trend most songs have either a C# or a G key. Moreover the majority of the keys used in both trend categories are A#, B, C, C#, G, and G# which all have a score of higher than 0.075 (purple line). The only huge dissimilarity between the trends that can be seen are within the key D: Dance tracks have a score of 0.1125 score whereas for Other tracks have only a 0.023 score. So just like the features Valence, Energy and Danceability from our emotions graph, the **Key** feature also has **no clear distinction** between the two trends. 

Luckily, there are still some other features available. The second row of plot shows a histogram of the different tempos in the corpus split up by trend. In general, the majority of the songs should be around [the preferred tempo](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.8019&rep=rep1&type=pdf) of 120-130 BPM. For both plots the mean is indicated with a purple line along with the respective mean of tempo. Although the means are quite close to one another, the distribution is quite different. We can see that the Dance tracks have a distribution similar to a Normal distribution. The Other tracks are more uniformly distributed. This means that we have found our **first difference** based on **tempo** between the two trends!

### A deep dive into **tempo**, **duration** and **volume**.{data-commentary-width=500px}

```{r}
mar <-get_playlist_audio_features("thesoundsofspotify","0IIt2QOYxRawDHQbZ8ln3M") %>%  add_audio_analysis()
apr <-get_playlist_audio_features("thesoundsofspotify","0PTklfVTalxE8j6T9NYZEG") %>%  add_audio_analysis()
may <-get_playlist_audio_features("thesoundsofspotify","093VTj7Esn9nYiFQFoa1YH") %>%  add_audio_analysis()
jun <-get_playlist_audio_features("thesoundsofspotify","2F3YMerEwMjXn3A4S7es6H") %>%  add_audio_analysis()
jul <-get_playlist_audio_features("thesoundsofspotify","4U9w6uCKivCsRLHFyZgkxI") %>%  add_audio_analysis()
aug <-get_playlist_audio_features("thesoundsofspotify","3zvNyJbb9rjdF0vag2O5xG") %>%  add_audio_analysis()
sep <-get_playlist_audio_features("thesoundsofspotify","2APegDsKij3yaL1ffG6Bz4") %>%  add_audio_analysis()
oct <-get_playlist_audio_features("thesoundsofspotify","4np9mWC6T4O8x7hoxTeUPP") %>%  add_audio_analysis()
nov <-get_playlist_audio_features("thesoundsofspotify","5xyKvHZIeWGAgWJ6ZEVm3H") %>%  add_audio_analysis()
dec <-get_playlist_audio_features("thesoundsofspotify","4n90Xr0lxy1yLhbzTAgzTx") %>%  add_audio_analysis()

corpus2 <- 
  bind_rows(
    mar %>% mutate(category = "Mar"),
    apr %>% mutate(category = "Apr"),
    may %>% mutate(category = "May"),
    jun %>% mutate(category = "Jun"),
    jul %>% mutate(category = "Jul"),
    aug %>% mutate(category = "Aug"),
    sep %>% mutate(category = "Sep"),
    oct %>% mutate(category = "Oct"),
    nov %>% mutate(category = "Nov"),
    dec %>% mutate(category = "Dec")
  )

# Delete duplicate songs
corpus2 <- corpus2[!duplicated(corpus2$track.name),]

corpus2$trend = c("dance","dance","dance","dance","transformation", "random", "dance","transformation","humour", 
"humour","dance","dance","humour","story telling","random","transformation","dance","humour","dance","dance",
"dance","dance","transformation","dance","humour","dance","transformation","dance", "dance","random","dance",
"dance","humour","random","story telling","dance","story telling","dance","dance","dance","dance","dance",
"humour","dance","story telling","humour","dance","random","dance","dance","dance","story telling","dance", 
"dance","dance","dance","dance","random","dance","story telling","transformation","dance","dance","random",
"dance","random","random","random","random","humour","dance","story telling","dance","story telling","transformation",
"random","random","dance","dance","random","story telling","dance","dance")

corpus2 <- corpus2 %>%
  mutate(trends = ifelse(trend == "dance", "Dance", "Other"))
```
```{r}

dummy <- data.frame(trends = c("Dance"), 
                    x = c(130),
                    y = c(4.1),
                    label = c("Candy Shop"))
corpus2 %>%
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) %>%
  unnest(sections) %>%
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = trends,
      alpha = loudness
    )
  ) +
  scale_color_manual(values = c("#9FBEE1", "#1E3D60"))+ 
  geom_point(aes(size = duration / 60)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  facet_wrap(~trends) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    size = "Duration (min)",
    alpha = "Volume (dBFS)"
  ) + 
  guides(colour = FALSE) 
```

***

Since tempo did show us a slight difference in distribution shape between the two trends, I have dissected this feature even further, by looking at the so called **section-level of tracks** from the Spotify API. 

The mean tempo has the same values as the previous tempo histogram. More interestingly, we can now also look into the **standard deviation** (SD) of the tempo. It might sound like a scary word, but in this case it indicates if there was a lot of **tempo variation within a track**. The Dance tracks usually have steady tempo that doesn't change as much throughout the track. The Other tracks actually have a slightly higher SD, around 0.5. So this actually indicates that there is a little bit more variation in tempo. One obvious outlier in the Dance category is the song *Candy shop*, apparently this song changes a lot tempo wise. The song with the most variation in the Other category is *Mr. Blue sky*. In addition, this track also has the highest mean tempo. 

The plot also shows **different sizing** of points. In the **Dance** category, there actually is **not much difference between sizes**. In the Other category, the dots are overall either slightly smaller or slightly bigger than the Dance tracks. If we look into the statistics, we can see that the mean of the duration for both trends lies around 3 minutes. However, the variation for Dance tracks is 0.67, whereas for Other tracks it is 0.90. This explains why we see **more smaller and larger points** in the **Other** plot. 

Lastly, the **opaqueness** of the points indicates the volume. There isn't anything that stands out as much. Although, there is one track in the Other category that is **very transparent**. This is the *Eine Kleine Nachtmusik K. 252. Allegro* song, again it proves itself to be an odd one out in the corpus. The other noticeable transparent dot, is also located in the Other category, around a mean of 145 BPM. This is the **Monkeys spinning monkeys** song that is yet again an outlier. 

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}
.tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0pky">Trend</th>
    <th class="tg-0pky">Feature</th>
    <th class="tg-0pky">Mean</th>
    <th class="tg-0pky">Standard Deviation</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-9wq8" rowspan="3"><br>Dance<br></td>
    <td class="tg-0pky">Duration</td>
    <td class="tg-0pky">181.04 s</td>
    <td class="tg-0pky">40.34 s</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="2">Volumne</td>
    <td class="tg-0pky" rowspan="2">-6.95 dBFS</td>
    <td class="tg-0pky" rowspan="2">2.77 dBFS</td>
  </tr>
  <tr>
  </tr>
  <tr>
    <td class="tg-lboi" rowspan="3">Other</td>
    <td class="tg-0pky">Duration</td>
    <td class="tg-0pky">179.33 s</td>
    <td class="tg-0pky">54.55 s</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="2">Volume</td>
    <td class="tg-0pky" rowspan="2">-7.93 dBFS</td>
    <td class="tg-0pky" rowspan="2">3.86 dBFS</td>
  </tr>
  <tr>
  </tr>
</tbody>
</table>

### Okay... So is there anything specific about these trends? Tell me what is the *trend* of this TikTok song?

```{r}
corpus_features <-
  corpus %>%  # For your portfolio, change this to the name of your corpus.
  add_audio_analysis() %>%
  mutate(
    playlist = factor(trends),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =  map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))
```
```{r}
corpus_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = corpus_features,          # Use the same name as the previous block.
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].
```
```{r}
corpus_cv <- corpus_features %>% vfold_cv(5)
```
```{r}
knn_model <-
  nearest_neighbor(neighbors = 1) %>%
  set_mode("classification") %>%
  set_engine("kknn")
corpus_knn <-
  workflow() %>%
  add_recipe(corpus_recipe) %>%
  add_model(knn_model) %>%
  fit_resamples(
    corpus_cv,
    control = control_resamples(save_pred = TRUE)
  )
```

```{r}
corpus_knn %>% get_conf_mat() %>% autoplot(type = "mosaic")
```

***

So what do we know so far? We know that, features such as Valence, Energy, Danceability, Key, Duration, and Loudness cannot really tell apart if a track is specifically a Dance track or a Other track. We do know that, the distribution of Tempo gives us a bit more insight on the differences between the two trends, but that is very minimal.

Here I ran some code to see if an **AI algorithm** - K-nearest neighbour (**KNN**) - was able to magically find out the differences between the two classes. The magic of the KNN algorithm happens when it has found a pattern within the data that you give it to identify different classes. Here the data were the available Spotify API features of tracks.

The plot shows how well the algorithm predicted the correct trends. When we look at the X-axis we see the label **Truth** which means that the songs in the left column all correspond to Dance tracks and on the right column to Other tracks. On the Y-axis we have the **Predicted** label. So we see that for the Dance tracks, a little bit more than half was correctly predicted as Dance, and the rest as Other. So based on this you would be able to say that it **predicted pretty well** for Dance tracks. However, if we look at the Truth column for Other tracks, we can see that the majority of those songs were **also predicted as Dance trakcs**. 

<!-- ### Wow I cannot choose man, help me grow this tree and maybe we can figure it out. -->

```{r}
# tree_model <-
#   decision_tree() %>%
#   set_mode("classification") %>% 
#   set_engine("C5.0")
# corpus_tree <- 
#   workflow() %>% 
#   add_recipe(corpus_recipe) %>% 
#   add_model(tree_model) %>% 
#   fit_resamples(
#     corpus_cv, 
#     control = control_resamples(save_pred = TRUE)
#   )
# corpus_tree %>% get_pr()
```

```{r}
# workflow() %>%
#   add_recipe(corpus_recipe) %>%
#   add_model(tree_model) %>%
#   fit(corpus_features) %>%
#   pluck("fit", "fit", "fit") %>%
#   summary()
```

```{r}
# forest_model <-
#   rand_forest() %>%
#   set_mode("classification") %>% 
#   set_engine("ranger", importance = "impurity")
# corpus_forest <- 
#   workflow() %>% 
#   add_recipe(corpus_recipe) %>% 
#   add_model(forest_model) %>% 
#   fit_resamples(
#     corpus_cv, 
#     control = control_resamples(save_pred = TRUE)
#   )
```

```{r}
# corpus_forest %>% get_pr()
```

```{r}
# workflow() %>%
#   add_recipe(corpus_recipe) %>%
#   add_model(forest_model) %>%
#   fit(corpus_features) %>%
#   pluck("fit", "fit", "fit") %>%
#   ranger::importance() %>%
#   enframe() %>%
#   mutate(name = fct_reorder(name, value)) %>%
#   ggplot(aes(name, value)) +
#   geom_col() +
#   coord_flip() +
#   theme_minimal() +
#   labs(x = NULL, y = "Importance")
```
```{r}
# what <- corpus_features %>%
#   filter(playlist == "Dance") 
# 
# 
# gaa <- corpus_features %>%
#   filter(playlist == "Other")
# 
# ac_12 <-corpus_features %>%
#   ggplot(aes(x = acousticness, y = c12, colour = playlist, size = energy)) +
#   geom_point(alpha = 0.8) +
#   scale_color_viridis_d() +
#   labs(
#     x = "Acousticness",
#     y = "12th Timbre component",
#     size = "Energy",
#     colour = "Playlist",
#     title = "Higest importance"
#   ) + 
#   geom_encircle(aes(group = playlist))
# 
# 
# temp_10 <- corpus_features %>%
#   ggplot(aes(x = E, y = F, colour = playlist, size = energy)) +
#   geom_point(alpha = 0.8) +
#   scale_color_viridis_d() +
#   labs(
#     x = "tempo",
#     y = "10th Timbre component",
#     size = "Energy",
#     colour = "Playlist",
#     title= "Lowest importance"
#   ) + 
#   geom_encircle(aes(group = playlist))
# 
# my_layout <- rbind(c(1,2), c(1,2))
# 
# grid.arrange(grobs = c(list(ac_12), list(temp_10)), layout_matrix = my_layout,
#              top = textGrob("Random forest",gp=gpar(fontsize=10,font=3)))
```


<!-- ***  -->

<!-- | Class | Precision | Recall | -->
<!-- |-------|-----------|--------| -->
<!-- | Dance | 0.590     | 0.750  | -->
<!-- | Other | 0.593     | 0.410  | -->

<!-- Expected *Highest importance* plot to have **more distinction** between the two trends *Dance* and *Other*. For the *Lowest importance* plot excepted to have  **no distinction** between the two trends. However, as you can see bu the encircled parts. there is a lot of overlap between *Dance* and *Other*, hence this classification does not really work, despite having relatively high importance for Acoustiness and the 12th timbre component (bove above a score of 2.0). However from the table we can also see that the classification model did not managed to find a pattern that makes tracks specifically a Dance or Other song.  --> 


### Conclusion
